{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 100,\n",
    "    'embedding_dim': 128,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.15,\n",
    "    'optimizer': 'RMSprop',\n",
    "    'num_layers': 3,\n",
    "    'num_heads': 2,\n",
    "    'context_size': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = cfg['learning_rate']\n",
    "EPOCHS = cfg['epochs']\n",
    "EMBEDDING_DIM = cfg['embedding_dim']\n",
    "BATCH_SIZE = cfg['batch_size']\n",
    "DROPOUT = cfg['dropout']\n",
    "OPTIMIZER = cfg['optimizer']\n",
    "NUM_LAYERS = cfg['num_layers']\n",
    "NUM_HEADS = cfg['num_heads']\n",
    "CONTEXT_SIZE = cfg['context_size']\n",
    "\n",
    "DIR = '/scratch/shu7bh/RES/2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "\n",
    "def clean_data_en(text: str) -> str:\n",
    "    text = normalize_unicode(text.lower().strip())\n",
    "    # text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_data_fr(text: str) -> str:\n",
    "    text = normalize_unicode(text.lower().strip())\n",
    "    # text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_data_en(text: str, unique_words_en: list) -> list:\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if unique_words_en is not None:\n",
    "        tokens = [token if token in unique_words_en else '<unk>' for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_data_fr(text: str, unique_words_fr: list) -> list:\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "\n",
    "    if unique_words_fr is not None:\n",
    "        tokens = [token if token in unique_words_fr else '<unk>' for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def read_data(path: str, unique_words_en: list, unique_words_fr: list):\n",
    "    data_en = []\n",
    "\n",
    "    with open(path + '.en', 'r') as f:\n",
    "        data_en = f.read().split('\\n')\n",
    "\n",
    "    data_en = [tokenize_data_en(clean_data_en(line), unique_words_en) for line in data_en]\n",
    "\n",
    "    data_fr = []\n",
    "\n",
    "    with open(path + '.fr', 'r') as f:\n",
    "        data_fr = f.read().split('\\n')\n",
    "\n",
    "    data_fr = [tokenize_data_fr(clean_data_fr(line), unique_words_fr) for line in data_fr]\n",
    "\n",
    "    return data_en, data_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, train_fr = read_data('data/train', None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_en = set()\n",
    "unique_words_fr = set()\n",
    "\n",
    "for line in train_en:\n",
    "    unique_words_en.update(line)\n",
    "\n",
    "for line in train_fr:\n",
    "    unique_words_fr.update(line)\n",
    "\n",
    "unique_words_en = list(unique_words_en)\n",
    "unique_words_fr = list(unique_words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_en, dev_fr = read_data('data/dev', unique_words_en, unique_words_fr)\n",
    "test_en, test_fr = read_data('data/test', unique_words_en, unique_words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(words_to_idx_en): 23608\n",
      "ic| len(words_to_idx_fr): 32500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_idx_en = {word: idx + 1 for idx, word in enumerate(unique_words_en)}\n",
    "\n",
    "words_to_idx_en['<pad>'] = 0\n",
    "words_to_idx_en['<unk>'] = len(words_to_idx_en)\n",
    "words_to_idx_en['<sos>'] = len(words_to_idx_en)\n",
    "words_to_idx_en['<eos>'] = len(words_to_idx_en)\n",
    "\n",
    "idx_to_words_en = {idx: word for word, idx in words_to_idx_en.items()}\n",
    "\n",
    "words_to_idx_fr = {word: idx + 1 for idx, word in enumerate(unique_words_fr)}\n",
    "\n",
    "words_to_idx_fr['<pad>'] = 0\n",
    "words_to_idx_fr['<unk>'] = len(words_to_idx_fr)\n",
    "words_to_idx_fr['<sos>'] = len(words_to_idx_fr)\n",
    "words_to_idx_fr['<eos>'] = len(words_to_idx_fr)\n",
    "\n",
    "idx_to_words_fr = {idx: word for word, idx in words_to_idx_fr.items()}\n",
    "\n",
    "ic(len(words_to_idx_en))\n",
    "ic(len(words_to_idx_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_idx_fr['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data_en, data_fr, words_to_idx_en, words_to_idx_fr):\n",
    "        self.data_en = []\n",
    "        self.data_fr = []\n",
    "        self.len_en = []\n",
    "        self.len_fr = []\n",
    "        \n",
    "        for sentence in data_en:\n",
    "            self.data_en.append(sentence[:CONTEXT_SIZE - 2])\n",
    "            self.len_en.append(len(self.data_en[-1]) + 2)\n",
    "\n",
    "        for sentence in data_fr:\n",
    "            self.data_fr.append(sentence[:CONTEXT_SIZE - 2])\n",
    "            self.len_fr.append(len(self.data_fr[-1]) + 2)\n",
    "\n",
    "        self.data_y = [[] for _ in range(len(self.data_fr))]\n",
    "\n",
    "        for i in range(len(self.data_en)):\n",
    "            self.data_en[i] = self.__add_padding(*self.__convert_to_tokens(self.data_en[i], words_to_idx_en))\n",
    "            self.data_fr[i] = self.__add_padding(*self.__convert_to_tokens(self.data_fr[i], words_to_idx_fr))\n",
    "            self.data_y[i]  = self.data_fr[i][1:] + [words_to_idx_fr['<pad>']]\n",
    "\n",
    "        self.data_en = torch.tensor(self.data_en)\n",
    "        self.data_fr = torch.tensor(self.data_fr)\n",
    "        self.data_y = torch.tensor(self.data_y)\n",
    "        self.len_en = torch.tensor(self.len_en)\n",
    "        self.len_fr = torch.tensor(self.len_fr)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_en)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en = self.data_en[idx]\n",
    "        fr = self.data_fr[idx]\n",
    "        y = self.data_y[idx]\n",
    "        len_en = self.len_en[idx]\n",
    "        len_fr = self.len_fr[idx]\n",
    "\n",
    "        return en, fr, y, len_en, len_fr\n",
    "\n",
    "    def __convert_to_tokens(self, sentence, words_to_idx):\n",
    "        return [words_to_idx['<sos>']] + [words_to_idx[word] for word in sentence] + [words_to_idx['<eos>']], words_to_idx\n",
    "    \n",
    "    def __add_padding(self, sentence, words_to_idx):\n",
    "        return sentence + [words_to_idx['<pad>']] * (CONTEXT_SIZE - len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_en, train_fr, words_to_idx_en, words_to_idx_fr)\n",
    "dev_dataset = TranslationDataset(dev_en, dev_fr, words_to_idx_en, words_to_idx_fr)\n",
    "test_dataset = TranslationDataset(test_en, test_fr, words_to_idx_en, words_to_idx_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Positional_Encoding(x, EMBEDDING_DIM, CONTEXT_SIZE):\n",
    "    pos = torch.arange(0, CONTEXT_SIZE, device=x.device).unsqueeze(1)\n",
    "\n",
    "    PE = torch.zeros(CONTEXT_SIZE, EMBEDDING_DIM, device=x.device)\n",
    "\n",
    "    PE[:, 0::2] = torch.sin(pos / (10000 ** (2 * torch.arange(0, EMBEDDING_DIM, 2, device=x.device) / EMBEDDING_DIM)))\n",
    "    PE[:, 1::2] = torch.cos(pos / (10000 ** (2 * torch.arange(1, EMBEDDING_DIM, 2, device=x.device) / EMBEDDING_DIM)))\n",
    "\n",
    "    PE = PE.unsqueeze(0)\n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, dropout: float, mask: bool) -> None:\n",
    "        \n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.mask = mask\n",
    "\n",
    "        self.W = nn.Linear(embedding_dim, 3 * embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        batch_size = x.size(0)\n",
    "        context_size = x.size(1)\n",
    "\n",
    "        qkv = self.W(x)\n",
    "        qkv = qkv.view(batch_size, context_size, 3, self.num_heads, self.embedding_dim // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = q @ k.permute(0, 1, 3, 2)\n",
    "        attn = attn / (self.embedding_dim ** 0.5)\n",
    "\n",
    "        mask = (torch.arange(context_size, device=l.device)[None, :] < l[:, None]).float().unsqueeze(1)\n",
    "        mask = mask.transpose(1, 2) @ mask\n",
    "\n",
    "        attn = attn.permute(1, 0, 2, 3)\n",
    "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = attn.permute(1, 0, 2, 3)\n",
    "\n",
    "        if self.mask:\n",
    "            mask = torch.tril(torch.ones(context_size, context_size, device=attn.device))[None, :, :]\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        attn = attn.nan_to_num()\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, context_size, self.embedding_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        context_size: int,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_self_attention = MultiHeadSelfAttention(embedding_dim, num_heads, dropout, mask=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim)\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input: tuple) -> torch.Tensor:\n",
    "        en, l = input\n",
    "        rc = en.clone()\n",
    "        en = self.multi_head_self_attention(en, l)\n",
    "        en = self.dropout(en)\n",
    "        en = self.layer_norm(en + rc)\n",
    "        rc = en.clone()\n",
    "        en = self.fc(en)\n",
    "        en = self.activation(en)\n",
    "        en = self.dropout(en)\n",
    "        en = self.layer_norm(en + rc)\n",
    "        return (en, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        context_size: int,\n",
    "        dropout: float,\n",
    "        filename: str = None\n",
    "    ) -> None:\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = Positional_Encoding\n",
    "        self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, context_size, dropout) for _ in range(num_layers)])\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if filename is not None:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def forward(self, en: torch.Tensor, l: torch.Tensor) -> torch.Tensor:\n",
    "        en = self.embedding(en)\n",
    "        en = en + self.positional_encoding(en, self.embedding_dim, self.context_size)\n",
    "        en, _ = self.layers((en, l))\n",
    "        return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(EncoderDecoderAttention, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.W_Q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.W_KV = nn.Linear(embedding_dim, 2 * embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, en: torch.Tensor, fr: torch.Tensor, l_en: torch.Tensor, l_fr: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = en.size(0)\n",
    "        context_size = en.size(1)\n",
    "\n",
    "        q = self.W_Q(fr).view(batch_size, context_size, self.num_heads, self.embedding_dim // self.num_heads).permute(0, 2, 1, 3)\n",
    "        kv = self.W_KV(en)\n",
    "        k, v = kv.view(batch_size, context_size, 2, self.num_heads, self.embedding_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        attn = q @ k.permute(0, 1, 3, 2)\n",
    "        attn = attn / (self.embedding_dim ** 0.5)\n",
    "\n",
    "        mask_en = (torch.arange(context_size, device=l_en.device)[None, :] < l_en[:, None]).float().unsqueeze(1)\n",
    "        mask_fr = (torch.arange(context_size, device=l_fr.device)[None, :] < l_fr[:, None]).float().unsqueeze(1)\n",
    "\n",
    "        mask = (mask_fr.transpose(1, 2) @ mask_en)\n",
    "\n",
    "        attn = attn.permute(1, 0, 2, 3)\n",
    "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = attn.permute(1, 0, 2, 3)\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = attn.nan_to_num()\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        en = attn @ v\n",
    "        en = en.permute(0, 2, 1, 3).contiguous()\n",
    "        en = en.view(batch_size, context_size, self.embedding_dim)\n",
    "\n",
    "        return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        context_size: int,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_self_attention = MultiHeadSelfAttention(embedding_dim, num_heads, dropout, mask=True)\n",
    "        self.encoder_decoder_attention = EncoderDecoderAttention(embedding_dim, num_heads, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim)\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input: tuple) -> torch.Tensor:\n",
    "        en, fr, l_en, l_fr = input\n",
    "        rc = fr.clone()\n",
    "        fr = self.multi_head_self_attention(fr, l_fr)\n",
    "        fr = self.dropout(fr)\n",
    "        fr = self.layer_norm(fr + rc)\n",
    "        rc = fr.clone()\n",
    "        fr = self.encoder_decoder_attention(en, fr, l_en, l_fr)\n",
    "        fr = self.dropout(fr)\n",
    "        fr = self.layer_norm(fr + rc)\n",
    "        rc = fr.clone()\n",
    "        fr = self.fc(fr)\n",
    "        fr = self.activation(fr)\n",
    "        fr = self.dropout(fr)\n",
    "        fr = self.layer_norm(fr + rc)\n",
    "        return (en, fr, l_en, l_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        context_size: int,\n",
    "        dropout: float,\n",
    "        filename: str = None\n",
    "    ) -> None:\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = Positional_Encoding\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embedding_dim, num_heads, context_size, dropout) for _ in range(num_layers)])\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if filename is not None:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def forward(self, en: torch.Tensor, fr: torch.Tensor, l_en: torch.Tensor, l_fr: torch.Tensor) -> torch.Tensor:\n",
    "        fr = self.embedding(fr)\n",
    "        fr = fr + self.positional_encoding(fr, self.embedding_dim, self.context_size)\n",
    "        _, fr, _, _ = self.layers((en, fr, l_en, l_fr))\n",
    "        return fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience:int = 3, delta:float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss:float = np.inf\n",
    "        self.best_model_pth = 0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, loss, epoch: int):\n",
    "        should_stop = False\n",
    "\n",
    "        if loss >= self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                should_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.best_model_pth = epoch\n",
    "        return should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size_en: int, vocab_size_fr: int, embedding_dim: int, num_heads: int, num_layers: int, context_size: int, dropout: float, filename: str = None) -> None:\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(vocab_size_en, embedding_dim, num_heads, num_layers, context_size, dropout, filename)\n",
    "        self.decoder = Decoder(vocab_size_fr, embedding_dim, num_heads, num_layers, context_size, dropout, filename)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size_fr)\n",
    "\n",
    "    def forward(self, en: torch.Tensor, fr: torch.Tensor, len_en: torch.Tensor, len_fr: torch.Tensor) -> torch.Tensor:\n",
    "        en = self.encoder(en, len_en)\n",
    "        en = self.decoder(en, fr, len_en, len_fr)\n",
    "        en = self.fc(en)\n",
    "        return en\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, validation_loader: DataLoader, epochs: int, learning_rate: float, optimizer: str, filename: str) -> None:\n",
    "        self.es = EarlyStopping()\n",
    "        self.optimizer = getattr(torch.optim, optimizer)(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch: {epoch + 1}/{epochs}')\n",
    "\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "            self.__train(train_loader)\n",
    "\n",
    "            self.criterion = nn.CrossEntropyLoss(ignore_index=words_to_idx_fr['<pad>'])\n",
    "            loss = self.__validate(validation_loader)\n",
    "\n",
    "            if self.es(loss, epoch):\n",
    "                break\n",
    "            if self.es.counter == 0:\n",
    "                torch.save(self.state_dict(), filename)\n",
    "\n",
    "\n",
    "    def __train(self, train_loader: DataLoader) -> None:\n",
    "        self.train()\n",
    "        total_loss = []\n",
    "\n",
    "        pbar = tqdm(train_loader, total=len(train_loader))\n",
    "        for en, fr, y, len_en, len_fr in pbar:\n",
    "            loss = self.__call(en, fr, y, len_en, len_fr)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pbar.set_description(f'T Loss: {loss.item():7.4f}, Avg Loss: {np.mean(total_loss):7.4f}')\n",
    "\n",
    "        # wandb.log({'train_loss': np.mean(total_loss)})\n",
    "\n",
    "    def __validate(self, validation_loader: DataLoader) -> None:\n",
    "        self.eval()\n",
    "        total_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(validation_loader, total=len(validation_loader))\n",
    "            for en, fr, y, len_en, len_fr in pbar:\n",
    "                loss = self.__call(en, fr, y, len_en, len_fr)\n",
    "                total_loss.append(loss.item())\n",
    "\n",
    "                pbar.set_description(f'V Loss: {loss.item():7.4f}, Avg Loss: {np.mean(total_loss):7.4f}, Counter: {self.es.counter}, Best Loss: {self.es.best_loss:7.4f}')\n",
    "\n",
    "        # wandb.log({'dev_loss': np.mean(total_loss)})\n",
    "        return np.mean(total_loss)\n",
    "\n",
    "    def __call(self, en: torch.Tensor, fr: torch.Tensor, y: torch.Tensor, len_en: torch.Tensor, len_fr: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        en = en.to(DEVICE)\n",
    "        fr = fr.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        len_en = len_en.to(DEVICE)\n",
    "        len_fr = len_fr.to(DEVICE)\n",
    "\n",
    "        output = self(en, fr, len_en, len_fr)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = self.criterion(output, y)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, loader: DataLoader, idx_to_words_fr: dict, idx_to_words_en: dict, filename: str) -> None:\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    inp = []\n",
    "    predicted = []\n",
    "    target = []\n",
    "    len_inp = []\n",
    "    len_y = []\n",
    "    model.criterion = nn.CrossEntropyLoss(ignore_index=words_to_idx_fr['<pad>'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, total=len(loader))\n",
    "        for en, fr, y, len_en, len_fr in pbar:\n",
    "            en = en.to(DEVICE)\n",
    "            fr = fr.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            len_en = len_en.to(DEVICE)\n",
    "            len_fr = len_fr.to(DEVICE)\n",
    "\n",
    "            output = model(en, fr, len_en, len_fr)\n",
    "\n",
    "            inp.extend(en.tolist())\n",
    "            predicted.extend(output.argmax(dim=-1).tolist())\n",
    "            target.extend(y.tolist())\n",
    "\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            y = y.view(-1)\n",
    "\n",
    "            loss = model.criterion(output, y)\n",
    "            total_loss.append(loss.item())\n",
    "            len_y.extend((len_fr - 2).tolist()) # 2 to remove the 1 extra <eos> and <pad> tokens\n",
    "            len_inp.extend((len_en - 2).tolist())\n",
    "\n",
    "            pbar.set_description(f'Loss: {np.mean(total_loss):7.4f}')\n",
    "\n",
    "    inp = [[idx_to_words_en[idx] for idx in sentence] for sentence in inp]\n",
    "    predicted = [[idx_to_words_fr[idx] for idx in sentence] for sentence in predicted]\n",
    "    target = [[idx_to_words_fr[idx] for idx in sentence] for sentence in target]\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        inp[i] = inp[i][1:len_inp[i] + 1]\n",
    "        predicted[i] = predicted[i][:len_y[i]]\n",
    "        target[i] = [target[i][:len_y[i]]]\n",
    "\n",
    "    with open(DIR + filename + '.bleu.txt', 'w') as f:\n",
    "        f.write('Input; Target; Predicted; BLEU Score\\n')\n",
    "        for i in range(len(predicted)):\n",
    "            f.write(f'''\"{' '.join(inp[i])}\"; ''')\n",
    "            f.write(f'''\"{' '.join(target[i][0])}\"; ''')\n",
    "            f.write(f'''\"{' '.join(predicted[i])}\"; ''')\n",
    "            sentence_bleu = bleu_score([predicted[i]], [target[i]])\n",
    "            f.write(f'{sentence_bleu:5.3f}\\n')\n",
    "\n",
    "    blue_metric = bleu_score(predicted, target)\n",
    "    print(f'BLEU Score: {blue_metric:7.2f}')\n",
    "\n",
    "    # wandb.log({'loss': np.mean(total_loss), 'bleu_score': blue_metric})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| Transformer(len(words_to_idx_en), len(words_to_idx_fr), EMBEDDING_DIM, NUM_HEADS, NUM_LAYERS, CONTEXT_SIZE, DROPOUT, filename=None).to(DEVICE): Transformer(\n",
      "                                                                                                                                                      (encoder): Encoder(\n",
      "                                                                                                                                                        (embedding): Embedding(23608, 128)\n",
      "                                                                                                                                                        (layers): Sequential(\n",
      "                                                                                                                                                          (0): EncoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=128, out_features=384, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                          (1): EncoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=128, out_features=384, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                          (2): EncoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=128, out_features=384, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                        )\n",
      "                                                                                                                                                      )\n",
      "                                                                                                                                                      (decoder): Decoder(\n",
      "                                                                                                                                                        (embedding): Embedding(32500, 128)\n",
      "                                                                                                                                                        (layers): Sequential(\n",
      "                                                                                                                                                          (0): DecoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=128, out_features=384, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (encoder_decoder_attention): EncoderDecoderAttention(\n",
      "                                                                                                                                                              (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "                                                                                                                                                              (W_KV): Linear(in_features=128, out_features=256, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                          (1): DecoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=128, out_features=384, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (encoder_decoder_attention): EncoderDecoderAttention(\n",
      "                                                                                                                                                              (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "                                                                                                                                                              (W_KV): Linear(in_features=128, out_features=256, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                          (2): DecoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=128, out_features=384, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (encoder_decoder_attention): EncoderDecoderAttention(\n",
      "                                                                                                                                                              (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
      "                                                                                                                                                              (W_KV): Linear(in_features=128, out_features=256, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.15, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                        )\n",
      "                                                                                                                                                      )\n",
      "                                                                                                                                                      (fc): Linear(in_features=128, out_features=32500, bias=True)\n",
      "                                                                                                                                                    )\n"
     ]
    }
   ],
   "source": [
    "model = ic(Transformer(len(words_to_idx_en), len(words_to_idx_fr), EMBEDDING_DIM, NUM_HEADS, NUM_LAYERS, CONTEXT_SIZE, DROPOUT, filename=None).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "Transformer                                        --\n",
       "Encoder: 1-1                                     --\n",
       "    Embedding: 2-1                              3,021,824\n",
       "    Sequential: 2-2                             --\n",
       "        EncoderLayer: 3-1                      181,504\n",
       "        EncoderLayer: 3-2                      181,504\n",
       "        EncoderLayer: 3-3                      181,504\n",
       "Decoder: 1-2                                     --\n",
       "    Embedding: 2-3                              4,160,000\n",
       "    Sequential: 2-4                             --\n",
       "        DecoderLayer: 3-4                      231,040\n",
       "        DecoderLayer: 3-5                      231,040\n",
       "        DecoderLayer: 3-6                      231,040\n",
       "Linear: 1-3                                      4,192,500\n",
       "===========================================================================\n",
       "Total params: 12,611,956\n",
       "Trainable params: 12,611,956\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.5656, Avg Loss:  1.8857: 100%|| 938/938 [00:37<00:00, 24.76it/s]\n",
      "V Loss:  5.3602, Avg Loss:  5.3211, Counter: 0, Best Loss:     inf: 100%|| 28/28 [00:00<00:00, 60.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.1647, Avg Loss:  1.5539: 100%|| 938/938 [00:35<00:00, 26.47it/s]\n",
      "V Loss:  5.0965, Avg Loss:  4.9470, Counter: 0, Best Loss:  5.3211: 100%|| 28/28 [00:00<00:00, 57.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.3465, Avg Loss:  1.4157: 100%|| 938/938 [00:35<00:00, 26.23it/s]\n",
      "V Loss:  5.2844, Avg Loss:  4.7792, Counter: 0, Best Loss:  4.9470: 100%|| 28/28 [00:00<00:00, 59.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2809, Avg Loss:  1.3199: 100%|| 938/938 [00:37<00:00, 24.97it/s]\n",
      "V Loss:  4.6108, Avg Loss:  4.7107, Counter: 0, Best Loss:  4.7792: 100%|| 28/28 [00:00<00:00, 60.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.3651, Avg Loss:  1.2410: 100%|| 938/938 [00:36<00:00, 25.52it/s]\n",
      "V Loss:  4.5181, Avg Loss:  4.6136, Counter: 0, Best Loss:  4.7107: 100%|| 28/28 [00:00<00:00, 59.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.0817, Avg Loss:  1.1743: 100%|| 938/938 [00:36<00:00, 25.41it/s]\n",
      "V Loss:  4.7516, Avg Loss:  4.5828, Counter: 0, Best Loss:  4.6136: 100%|| 28/28 [00:00<00:00, 60.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.6940, Avg Loss:  1.1186: 100%|| 938/938 [00:37<00:00, 25.18it/s]\n",
      "V Loss:  4.6830, Avg Loss:  4.5448, Counter: 0, Best Loss:  4.5828: 100%|| 28/28 [00:00<00:00, 64.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7662, Avg Loss:  1.0708: 100%|| 938/938 [00:35<00:00, 26.75it/s]\n",
      "V Loss:  4.7246, Avg Loss:  4.4735, Counter: 0, Best Loss:  4.5448: 100%|| 28/28 [00:00<00:00, 54.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.8744, Avg Loss:  1.0304: 100%|| 938/938 [00:37<00:00, 25.19it/s]\n",
      "V Loss:  4.7180, Avg Loss:  4.5090, Counter: 0, Best Loss:  4.4735: 100%|| 28/28 [00:00<00:00, 61.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.0406, Avg Loss:  0.9970: 100%|| 938/938 [00:36<00:00, 25.68it/s]\n",
      "V Loss:  4.6450, Avg Loss:  4.4612, Counter: 1, Best Loss:  4.4735: 100%|| 28/28 [00:00<00:00, 61.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.8344, Avg Loss:  0.9671: 100%|| 938/938 [00:36<00:00, 25.89it/s]\n",
      "V Loss:  4.8968, Avg Loss:  4.4830, Counter: 0, Best Loss:  4.4612: 100%|| 28/28 [00:00<00:00, 61.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7236, Avg Loss:  0.9403: 100%|| 938/938 [00:36<00:00, 25.74it/s]\n",
      "V Loss:  4.6668, Avg Loss:  4.4872, Counter: 1, Best Loss:  4.4612: 100%|| 28/28 [00:00<00:00, 60.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.9253, Avg Loss:  0.9167: 100%|| 938/938 [00:37<00:00, 25.20it/s]\n",
      "V Loss:  4.0395, Avg Loss:  4.4444, Counter: 2, Best Loss:  4.4612: 100%|| 28/28 [00:00<00:00, 62.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7750, Avg Loss:  0.8961: 100%|| 938/938 [00:36<00:00, 25.45it/s]\n",
      "V Loss:  4.7306, Avg Loss:  4.4780, Counter: 0, Best Loss:  4.4444: 100%|| 28/28 [00:00<00:00, 60.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7746, Avg Loss:  0.8751: 100%|| 938/938 [00:35<00:00, 26.51it/s]\n",
      "V Loss:  4.1250, Avg Loss:  4.4424, Counter: 1, Best Loss:  4.4444: 100%|| 28/28 [00:00<00:00, 60.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2622, Avg Loss:  0.8571: 100%|| 938/938 [00:37<00:00, 24.98it/s]\n",
      "V Loss:  4.3210, Avg Loss:  4.4644, Counter: 0, Best Loss:  4.4424: 100%|| 28/28 [00:00<00:00, 59.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.8691, Avg Loss:  0.8403: 100%|| 938/938 [00:37<00:00, 25.05it/s]\n",
      "V Loss:  4.0380, Avg Loss:  4.4159, Counter: 1, Best Loss:  4.4424: 100%|| 28/28 [00:00<00:00, 66.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2350, Avg Loss:  0.8240: 100%|| 938/938 [00:37<00:00, 24.95it/s]\n",
      "V Loss:  4.0166, Avg Loss:  4.4483, Counter: 0, Best Loss:  4.4159: 100%|| 28/28 [00:00<00:00, 60.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2187, Avg Loss:  0.8090: 100%|| 938/938 [00:36<00:00, 25.56it/s]\n",
      "V Loss:  4.6694, Avg Loss:  4.5179, Counter: 1, Best Loss:  4.4159: 100%|| 28/28 [00:00<00:00, 61.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.8645, Avg Loss:  0.7937: 100%|| 938/938 [00:37<00:00, 24.99it/s]\n",
      "V Loss:  4.2187, Avg Loss:  4.4368, Counter: 2, Best Loss:  4.4159: 100%|| 28/28 [00:00<00:00, 63.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.0417, Avg Loss:  0.7800: 100%|| 938/938 [00:36<00:00, 25.48it/s]\n",
      "V Loss:  4.9172, Avg Loss:  4.4410, Counter: 3, Best Loss:  4.4159: 100%|| 28/28 [00:00<00:00, 61.13it/s]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_loader, dev_loader, EPOCHS, LEARNING_RATE, OPTIMIZER, os.path.join(DIR, 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:  1.8834: 100%|| 938/938 [00:15<00:00, 61.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:    0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:  4.4174: 100%|| 28/28 [00:00<00:00, 56.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:    0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:  3.9206: 100%|| 41/41 [00:00<00:00, 63.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:    0.15\n"
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "model.load_state_dict(torch.load(os.path.join(DIR, 'best_model.pth')))\n",
    "evaluate_metrics(model, train_loader, idx_to_words_fr, idx_to_words_en, 'train')\n",
    "evaluate_metrics(model, dev_loader, idx_to_words_fr, idx_to_words_en, 'dev')\n",
    "evaluate_metrics(model, test_loader, idx_to_words_fr, idx_to_words_en, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
