{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 100,\n",
    "    'embedding_dim': 50,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.2,\n",
    "    'optimizer': 'Adam',\n",
    "    'num_layers': 2,\n",
    "    'num_heads': 2,\n",
    "    'context_size': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = {\n",
    "#     'method': 'random',\n",
    "#     'name': 'Transformer_Translation',\n",
    "#     'metric': {\n",
    "#         'goal': 'maximize',\n",
    "#         'name': 'blue_score'\n",
    "#     },\n",
    "#     'parameters': {\n",
    "#         'learning_rate': { 'values': [0.0005, 0.001] },\n",
    "#         'epochs': { 'value': 100 },\n",
    "#         'embedding_dim': { 'value': 128 },\n",
    "#         'batch_size': { 'value': 32 },\n",
    "#         'dropout': { 'values': [0.0, 0.15, 0.3] },\n",
    "#         'optimizer': { 'values': ['Adam', 'RMSprop'] },\n",
    "#         'num_layers': { 'values': [2, 3] },\n",
    "#         'num_heads': { 'values': [2, 4, 8] },\n",
    "#         'context_size': { 'value': 64 }\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = cfg['learning_rate']\n",
    "EPOCHS = cfg['epochs']\n",
    "EMBEDDING_DIM = cfg['embedding_dim']\n",
    "BATCH_SIZE = cfg['batch_size']\n",
    "DROPOUT = cfg['dropout']\n",
    "OPTIMIZER = cfg['optimizer']\n",
    "NUM_LAYERS = cfg['num_layers']\n",
    "NUM_HEADS = cfg['num_heads']\n",
    "CONTEXT_SIZE = cfg['context_size']\n",
    "\n",
    "DIR = '/scratch/shu7bh/RES/2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = cfg['parameters']['epochs']['value']\n",
    "# BATCH_SIZE = cfg['parameters']['batch_size']['value']\n",
    "# CONTEXT_SIZE = cfg['parameters']['context_size']['value']\n",
    "# EMBEDDING_DIM = cfg['parameters']['embedding_dim']['value']\n",
    "# OPTIMIZER = cfg['parameters']['optimizer']['values'][0]\n",
    "# LEARNING_RATE = cfg['parameters']['learning_rate']['values'][0]\n",
    "# DROPOUT = cfg['parameters']['dropout']['values'][0]\n",
    "# NUM_LAYERS = cfg['parameters']['num_layers']['values'][0]\n",
    "# NUM_HEADS = cfg['parameters']['num_heads']['values'][0]\n",
    "\n",
    "# DIR = '/scratch/shu7bh/RES/2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "\n",
    "def clean_data_en(text: str) -> str:\n",
    "    text = normalize_unicode(text.lower().strip())\n",
    "    # text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_data_fr(text: str) -> str:\n",
    "    text = normalize_unicode(text.lower().strip())\n",
    "    # text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize_data_en(text: str, unique_words_en: list) -> list:\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if unique_words_en is not None:\n",
    "        tokens = [token if token in unique_words_en else '<unk>' for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_data_fr(text: str, unique_words_fr: list) -> list:\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "\n",
    "    if unique_words_fr is not None:\n",
    "        tokens = [token if token in unique_words_fr else '<unk>' for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def read_data(path: str, unique_words_en: list, unique_words_fr: list):\n",
    "    data_en = []\n",
    "\n",
    "    with open(path + '.en', 'r') as f:\n",
    "        data_en = f.read().split('\\n')\n",
    "\n",
    "    data_en = [tokenize_data_en(clean_data_en(line), unique_words_en) for line in data_en]\n",
    "\n",
    "    data_fr = []\n",
    "\n",
    "    with open(path + '.fr', 'r') as f:\n",
    "        data_fr = f.read().split('\\n')\n",
    "\n",
    "    data_fr = [tokenize_data_fr(clean_data_fr(line), unique_words_fr) for line in data_fr]\n",
    "\n",
    "    return data_en, data_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, train_fr = read_data('data/train', None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_en = set()\n",
    "unique_words_fr = set()\n",
    "\n",
    "for line in train_en:\n",
    "    unique_words_en.update(line)\n",
    "\n",
    "for line in train_fr:\n",
    "    unique_words_fr.update(line)\n",
    "\n",
    "unique_words_en = list(unique_words_en)\n",
    "unique_words_fr = list(unique_words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_en, dev_fr = read_data('data/dev', unique_words_en, unique_words_fr)\n",
    "test_en, test_fr = read_data('data/test', unique_words_en, unique_words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(words_to_idx_en): 23608\n",
      "ic| len(words_to_idx_fr): 32500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32500"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_idx_en = {word: idx + 1 for idx, word in enumerate(unique_words_en)}\n",
    "\n",
    "words_to_idx_en['<pad>'] = 0\n",
    "words_to_idx_en['<unk>'] = len(words_to_idx_en)\n",
    "words_to_idx_en['<sos>'] = len(words_to_idx_en)\n",
    "words_to_idx_en['<eos>'] = len(words_to_idx_en)\n",
    "\n",
    "idx_to_words_en = {idx: word for word, idx in words_to_idx_en.items()}\n",
    "\n",
    "words_to_idx_fr = {word: idx + 1 for idx, word in enumerate(unique_words_fr)}\n",
    "\n",
    "words_to_idx_fr['<pad>'] = 0\n",
    "words_to_idx_fr['<unk>'] = len(words_to_idx_fr)\n",
    "words_to_idx_fr['<sos>'] = len(words_to_idx_fr)\n",
    "words_to_idx_fr['<eos>'] = len(words_to_idx_fr)\n",
    "\n",
    "idx_to_words_fr = {idx: word for word, idx in words_to_idx_fr.items()}\n",
    "\n",
    "ic(len(words_to_idx_en))\n",
    "ic(len(words_to_idx_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_idx_fr['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data_en, data_fr, words_to_idx_en, words_to_idx_fr):\n",
    "        self.data_en = []\n",
    "        self.data_fr = []\n",
    "        self.len_en = []\n",
    "        self.len_fr = []\n",
    "        \n",
    "        for sentence in data_en:\n",
    "            self.data_en.append(sentence[:CONTEXT_SIZE - 2])\n",
    "            self.len_en.append(len(self.data_en[-1]) + 2)\n",
    "\n",
    "        for sentence in data_fr:\n",
    "            self.data_fr.append(sentence[:CONTEXT_SIZE - 2])\n",
    "            self.len_fr.append(len(self.data_fr[-1]) + 2)\n",
    "\n",
    "        self.data_y = [[] for _ in range(len(self.data_fr))]\n",
    "\n",
    "        for i in range(len(self.data_en)):\n",
    "            self.data_en[i] = self.__add_padding(*self.__convert_to_tokens(self.data_en[i], words_to_idx_en))\n",
    "            self.data_fr[i] = self.__add_padding(*self.__convert_to_tokens(self.data_fr[i], words_to_idx_fr))\n",
    "            self.data_y[i]  = self.data_fr[i][1:] + [words_to_idx_fr['<pad>']]\n",
    "\n",
    "        self.data_en = torch.tensor(self.data_en)\n",
    "        self.data_fr = torch.tensor(self.data_fr)\n",
    "        self.data_y = torch.tensor(self.data_y)\n",
    "        self.len_en = torch.tensor(self.len_en)\n",
    "        self.len_fr = torch.tensor(self.len_fr)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_en)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en = self.data_en[idx]\n",
    "        fr = self.data_fr[idx]\n",
    "        y = self.data_y[idx]\n",
    "        len_en = self.len_en[idx]\n",
    "        len_fr = self.len_fr[idx]\n",
    "\n",
    "        return en, fr, y, len_en, len_fr\n",
    "\n",
    "    def __convert_to_tokens(self, sentence, words_to_idx):\n",
    "        return [words_to_idx['<sos>']] + [words_to_idx[word] for word in sentence] + [words_to_idx['<eos>']], words_to_idx\n",
    "    \n",
    "    def __add_padding(self, sentence, words_to_idx):\n",
    "        return sentence + [words_to_idx['<pad>']] * (CONTEXT_SIZE - len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_en, train_fr, words_to_idx_en, words_to_idx_fr)\n",
    "dev_dataset = TranslationDataset(dev_en, dev_fr, words_to_idx_en, words_to_idx_fr)\n",
    "test_dataset = TranslationDataset(test_en, test_fr, words_to_idx_en, words_to_idx_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Positional_Encoding(x, EMBEDDING_DIM, CONTEXT_SIZE):\n",
    "    pos = torch.arange(0, CONTEXT_SIZE, device=x.device).unsqueeze(1)\n",
    "\n",
    "    PE = torch.zeros(CONTEXT_SIZE, EMBEDDING_DIM, device=x.device)\n",
    "\n",
    "    PE[:, 0::2] = torch.sin(pos / (10000 ** (2 * torch.arange(0, EMBEDDING_DIM, 2, device=x.device) / EMBEDDING_DIM)))\n",
    "    PE[:, 1::2] = torch.cos(pos / (10000 ** (2 * torch.arange(1, EMBEDDING_DIM, 2, device=x.device) / EMBEDDING_DIM)))\n",
    "\n",
    "    PE = PE.unsqueeze(0)\n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, dropout: float, mask: bool) -> None:\n",
    "        \n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.mask = mask\n",
    "\n",
    "        self.W = nn.Linear(embedding_dim, 3 * embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        batch_size = x.size(0)\n",
    "        context_size = x.size(1)\n",
    "\n",
    "        qkv = self.W(x)\n",
    "        qkv = qkv.view(batch_size, context_size, 3, self.num_heads, self.embedding_dim // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = q @ k.permute(0, 1, 3, 2)\n",
    "        attn = attn / (self.embedding_dim ** 0.5)\n",
    "\n",
    "        mask = (torch.arange(context_size, device=l.device)[None, :] < l[:, None]).float().unsqueeze(1)\n",
    "        mask = mask.transpose(1, 2) @ mask\n",
    "\n",
    "        attn = attn.permute(1, 0, 2, 3)\n",
    "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = attn.permute(1, 0, 2, 3)\n",
    "\n",
    "        if self.mask:\n",
    "            mask = torch.tril(torch.ones(context_size, context_size, device=attn.device))[None, :, :]\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        attn = attn.nan_to_num()\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, context_size, self.embedding_dim)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        context_size: int,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_self_attention = MultiHeadSelfAttention(embedding_dim, num_heads, dropout, mask=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim)\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input: tuple) -> torch.Tensor:\n",
    "        en, l = input\n",
    "        rc = en.clone()\n",
    "        en = self.multi_head_self_attention(en, l)\n",
    "        en = self.dropout(en)\n",
    "        en = self.layer_norm(en + rc)\n",
    "        rc = en.clone()\n",
    "        en = self.fc(en)\n",
    "        en = self.activation(en)\n",
    "        en = self.dropout(en)\n",
    "        en = self.layer_norm(en + rc)\n",
    "        return (en, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        context_size: int,\n",
    "        dropout: float,\n",
    "        filename: str = None\n",
    "    ) -> None:\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = Positional_Encoding\n",
    "        self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, context_size, dropout) for _ in range(num_layers)])\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if filename is not None:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def forward(self, en: torch.Tensor, l: torch.Tensor) -> torch.Tensor:\n",
    "        en = self.embedding(en)\n",
    "        en = en + self.positional_encoding(en, self.embedding_dim, self.context_size)\n",
    "        en, _ = self.layers((en, l))\n",
    "        return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(EncoderDecoderAttention, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.W_Q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.W_KV = nn.Linear(embedding_dim, 2 * embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, en: torch.Tensor, fr: torch.Tensor, l_en: torch.Tensor, l_fr: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = en.size(0)\n",
    "        context_size = en.size(1)\n",
    "\n",
    "        q = self.W_Q(fr).view(batch_size, context_size, self.num_heads, self.embedding_dim // self.num_heads).permute(0, 2, 1, 3)\n",
    "        kv = self.W_KV(en)\n",
    "        k, v = kv.view(batch_size, context_size, 2, self.num_heads, self.embedding_dim // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        attn = q @ k.permute(0, 1, 3, 2)\n",
    "        attn = attn / (self.embedding_dim ** 0.5)\n",
    "\n",
    "        mask_en = (torch.arange(context_size, device=l_en.device)[None, :] < l_en[:, None]).float().unsqueeze(1)\n",
    "        mask_fr = (torch.arange(context_size, device=l_fr.device)[None, :] < l_fr[:, None]).float().unsqueeze(1)\n",
    "\n",
    "        mask = (mask_fr.transpose(1, 2) @ mask_en)\n",
    "\n",
    "        attn = attn.permute(1, 0, 2, 3)\n",
    "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = attn.permute(1, 0, 2, 3)\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = attn.nan_to_num()\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        en = attn @ v\n",
    "        en = en.permute(0, 2, 1, 3).contiguous()\n",
    "        en = en.view(batch_size, context_size, self.embedding_dim)\n",
    "\n",
    "        return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        context_size: int,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_self_attention = MultiHeadSelfAttention(embedding_dim, num_heads, dropout, mask=True)\n",
    "        self.encoder_decoder_attention = EncoderDecoderAttention(embedding_dim, num_heads, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim)\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input: tuple) -> torch.Tensor:\n",
    "        en, fr, l_en, l_fr = input\n",
    "        rc = fr.clone()\n",
    "        fr = self.multi_head_self_attention(fr, l_fr)\n",
    "        fr = self.dropout(fr)\n",
    "        fr = self.layer_norm(fr + rc)\n",
    "        rc = fr.clone()\n",
    "        fr = self.encoder_decoder_attention(en, fr, l_en, l_fr)\n",
    "        fr = self.dropout(fr)\n",
    "        fr = self.layer_norm(fr + rc)\n",
    "        rc = fr.clone()\n",
    "        fr = self.fc(fr)\n",
    "        fr = self.activation(fr)\n",
    "        fr = self.dropout(fr)\n",
    "        fr = self.layer_norm(fr + rc)\n",
    "        return (en, fr, l_en, l_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        context_size: int,\n",
    "        dropout: float,\n",
    "        filename: str = None\n",
    "    ) -> None:\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = Positional_Encoding\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embedding_dim, num_heads, context_size, dropout) for _ in range(num_layers)])\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        if filename is not None:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def forward(self, en: torch.Tensor, fr: torch.Tensor, l_en: torch.Tensor, l_fr: torch.Tensor) -> torch.Tensor:\n",
    "        fr = self.embedding(fr)\n",
    "        fr = fr + self.positional_encoding(fr, self.embedding_dim, self.context_size)\n",
    "        _, fr, _, _ = self.layers((en, fr, l_en, l_fr))\n",
    "        return fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience:int = 3, delta:float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss:float = np.inf\n",
    "        self.best_model_pth = 0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, loss, epoch: int):\n",
    "        should_stop = False\n",
    "\n",
    "        if loss >= self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                should_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.best_model_pth = epoch\n",
    "        return should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size_en: int, vocab_size_fr: int, embedding_dim: int, num_heads: int, num_layers: int, context_size: int, dropout: float, filename: str = None) -> None:\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(vocab_size_en, embedding_dim, num_heads, num_layers, context_size, dropout, filename)\n",
    "        self.decoder = Decoder(vocab_size_fr, embedding_dim, num_heads, num_layers, context_size, dropout, filename)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size_fr)\n",
    "\n",
    "    def forward(self, en: torch.Tensor, fr: torch.Tensor, len_en: torch.Tensor, len_fr: torch.Tensor) -> torch.Tensor:\n",
    "        en = self.encoder(en, len_en)\n",
    "        en = self.decoder(en, fr, len_en, len_fr)\n",
    "        en = self.fc(en)\n",
    "        return en\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, validation_loader: DataLoader, epochs: int, learning_rate: float, optimizer: str, filename: str) -> None:\n",
    "        self.es = EarlyStopping()\n",
    "        self.optimizer = getattr(torch.optim, optimizer)(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch: {epoch + 1}/{epochs}')\n",
    "\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "            self.__train(train_loader)\n",
    "\n",
    "            self.criterion = nn.CrossEntropyLoss(ignore_index=words_to_idx_fr['<pad>'])\n",
    "            loss = self.__validate(validation_loader)\n",
    "\n",
    "            if self.es(loss, epoch):\n",
    "                break\n",
    "            if self.es.counter == 0:\n",
    "                torch.save(self.state_dict(), filename)\n",
    "\n",
    "\n",
    "    def __train(self, train_loader: DataLoader) -> None:\n",
    "        self.train()\n",
    "        total_loss = []\n",
    "\n",
    "        pbar = tqdm(train_loader, total=len(train_loader))\n",
    "        for en, fr, y, len_en, len_fr in pbar:\n",
    "            loss = self.__call(en, fr, y, len_en, len_fr)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pbar.set_description(f'T Loss: {loss.item():7.4f}, Avg Loss: {np.mean(total_loss):7.4f}')\n",
    "\n",
    "        # wandb.log({'train_loss': np.mean(total_loss)})\n",
    "\n",
    "    def __validate(self, validation_loader: DataLoader) -> None:\n",
    "        self.eval()\n",
    "        total_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(validation_loader, total=len(validation_loader))\n",
    "            for en, fr, y, len_en, len_fr in pbar:\n",
    "                loss = self.__call(en, fr, y, len_en, len_fr)\n",
    "                total_loss.append(loss.item())\n",
    "\n",
    "                pbar.set_description(f'V Loss: {loss.item():7.4f}, Avg Loss: {np.mean(total_loss):7.4f}, Counter: {self.es.counter}, Best Loss: {self.es.best_loss:7.4f}')\n",
    "\n",
    "        # wandb.log({'dev_loss': np.mean(total_loss)})\n",
    "        return np.mean(total_loss)\n",
    "\n",
    "    def __call(self, en: torch.Tensor, fr: torch.Tensor, y: torch.Tensor, len_en: torch.Tensor, len_fr: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        en = en.to(DEVICE)\n",
    "        fr = fr.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        len_en = len_en.to(DEVICE)\n",
    "        len_fr = len_fr.to(DEVICE)\n",
    "\n",
    "        output = self(en, fr, len_en, len_fr)\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = self.criterion(output, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate_metrics(self, test_loader: DataLoader, idx_to_words_fr: dict) -> None:\n",
    "        self.eval()\n",
    "        total_loss = []\n",
    "        predicted = []\n",
    "        target = []\n",
    "        len_y = []\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=words_to_idx_fr['<pad>'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(test_loader, total=len(test_loader))\n",
    "            for en, fr, y, len_en, len_fr in pbar:\n",
    "                en = en.to(DEVICE)\n",
    "                fr = fr.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                len_en = len_en.to(DEVICE)\n",
    "                len_fr = len_fr.to(DEVICE)\n",
    "\n",
    "                output = self(en, fr, len_en, len_fr)\n",
    "\n",
    "                predicted.extend(output.argmax(dim=-1).tolist())\n",
    "                target.extend(y.tolist())\n",
    "\n",
    "                output = output.view(-1, output.size(-1))\n",
    "                y = y.view(-1)\n",
    "\n",
    "                loss = self.criterion(output, y)\n",
    "                total_loss.append(loss.item())\n",
    "                len_y.extend((len_fr - 2).tolist()) # 2 to remove the 1 extra <eos> and <pad> tokens\n",
    "\n",
    "                pbar.set_description(f'Loss: {np.mean(total_loss):7.4f}')\n",
    "\n",
    "        predicted = [[idx_to_words_fr[idx] for idx in sentence] for sentence in predicted]\n",
    "        target = [[idx_to_words_fr[idx] for idx in sentence] for sentence in target]\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            predicted[i] = predicted[i][:len_y[i]]\n",
    "            target[i] = [target[i][:len_y[i]]]\n",
    "\n",
    "        blue_metric = bleu_score(predicted, target)\n",
    "        print(f'BLEU Score: {blue_metric:7.2f}')\n",
    "\n",
    "        # wandb.log({'loss': np.mean(total_loss), 'bleu_score': blue_metric})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| Transformer(len(words_to_idx_en), len(words_to_idx_fr), EMBEDDING_DIM, NUM_HEADS, NUM_LAYERS, CONTEXT_SIZE, DROPOUT, filename=None).to(DEVICE): Transformer(\n",
      "                                                                                                                                                      (encoder): Encoder(\n",
      "                                                                                                                                                        (embedding): Embedding(23608, 50)\n",
      "                                                                                                                                                        (layers): Sequential(\n",
      "                                                                                                                                                          (0): EncoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=50, out_features=150, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=50, out_features=200, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=200, out_features=50, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                          (1): EncoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=50, out_features=150, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=50, out_features=200, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=200, out_features=50, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                        )\n",
      "                                                                                                                                                      )\n",
      "                                                                                                                                                      (decoder): Decoder(\n",
      "                                                                                                                                                        (embedding): Embedding(32500, 50)\n",
      "                                                                                                                                                        (layers): Sequential(\n",
      "                                                                                                                                                          (0): DecoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=50, out_features=150, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (encoder_decoder_attention): EncoderDecoderAttention(\n",
      "                                                                                                                                                              (W_Q): Linear(in_features=50, out_features=50, bias=True)\n",
      "                                                                                                                                                              (W_KV): Linear(in_features=50, out_features=100, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=50, out_features=200, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=200, out_features=50, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                          (1): DecoderLayer(\n",
      "                                                                                                                                                            (multi_head_self_attention): MultiHeadSelfAttention(\n",
      "                                                                                                                                                              (W): Linear(in_features=50, out_features=150, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (encoder_decoder_attention): EncoderDecoderAttention(\n",
      "                                                                                                                                                              (W_Q): Linear(in_features=50, out_features=50, bias=True)\n",
      "                                                                                                                                                              (W_KV): Linear(in_features=50, out_features=100, bias=True)\n",
      "                                                                                                                                                              (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (dropout): Dropout(p=0.2, inplace=False)\n",
      "                                                                                                                                                            (layer_norm): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "                                                                                                                                                            (fc): Sequential(\n",
      "                                                                                                                                                              (0): Linear(in_features=50, out_features=200, bias=True)\n",
      "                                                                                                                                                              (1): ReLU()\n",
      "                                                                                                                                                              (2): Linear(in_features=200, out_features=50, bias=True)\n",
      "                                                                                                                                                            )\n",
      "                                                                                                                                                            (activation): ReLU()\n",
      "                                                                                                                                                          )\n",
      "                                                                                                                                                        )\n",
      "                                                                                                                                                      )\n",
      "                                                                                                                                                      (fc): Linear(in_features=50, out_features=32500, bias=True)\n",
      "                                                                                                                                                    )\n"
     ]
    }
   ],
   "source": [
    "model = ic(Transformer(len(words_to_idx_en), len(words_to_idx_fr), EMBEDDING_DIM, NUM_HEADS, NUM_LAYERS, CONTEXT_SIZE, DROPOUT, filename=None).to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "Transformer                                        --\n",
       "├─Encoder: 1-1                                     --\n",
       "│    └─Embedding: 2-1                              1,180,400\n",
       "│    └─Sequential: 2-2                             --\n",
       "│    │    └─EncoderLayer: 3-1                      28,000\n",
       "│    │    └─EncoderLayer: 3-2                      28,000\n",
       "├─Decoder: 1-2                                     --\n",
       "│    └─Embedding: 2-3                              1,625,000\n",
       "│    └─Sequential: 2-4                             --\n",
       "│    │    └─DecoderLayer: 3-3                      35,650\n",
       "│    │    └─DecoderLayer: 3-4                      35,650\n",
       "├─Linear: 1-3                                      1,657,500\n",
       "===========================================================================\n",
       "Total params: 4,590,200\n",
       "Trainable params: 4,590,200\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  2.1091, Avg Loss:  2.4256: 100%|██████████| 938/938 [00:29<00:00, 31.91it/s]\n",
      "V Loss:  5.3621, Avg Loss:  5.6348, Counter: 0, Best Loss:     inf: 100%|██████████| 28/28 [00:00<00:00, 74.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2102, Avg Loss:  1.7246: 100%|██████████| 938/938 [00:29<00:00, 31.97it/s]\n",
      "V Loss:  5.2971, Avg Loss:  5.3427, Counter: 0, Best Loss:  5.6348: 100%|██████████| 28/28 [00:00<00:00, 79.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.6407, Avg Loss:  1.5980: 100%|██████████| 938/938 [00:28<00:00, 32.40it/s]\n",
      "V Loss:  4.8469, Avg Loss:  5.1589, Counter: 0, Best Loss:  5.3427: 100%|██████████| 28/28 [00:00<00:00, 78.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  2.1715, Avg Loss:  1.5097: 100%|██████████| 938/938 [00:29<00:00, 32.09it/s]\n",
      "V Loss:  5.0391, Avg Loss:  5.0591, Counter: 0, Best Loss:  5.1589: 100%|██████████| 28/28 [00:00<00:00, 75.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2365, Avg Loss:  1.4355: 100%|██████████| 938/938 [00:29<00:00, 31.95it/s]\n",
      "V Loss:  4.9786, Avg Loss:  5.0168, Counter: 0, Best Loss:  5.0591: 100%|██████████| 28/28 [00:00<00:00, 75.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.6669, Avg Loss:  1.3709: 100%|██████████| 938/938 [00:30<00:00, 31.17it/s]\n",
      "V Loss:  4.9025, Avg Loss:  4.9774, Counter: 0, Best Loss:  5.0168: 100%|██████████| 28/28 [00:00<00:00, 75.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.5005, Avg Loss:  1.3129: 100%|██████████| 938/938 [00:29<00:00, 31.88it/s]\n",
      "V Loss:  4.8819, Avg Loss:  4.9717, Counter: 0, Best Loss:  4.9774: 100%|██████████| 28/28 [00:00<00:00, 75.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2625, Avg Loss:  1.2604: 100%|██████████| 938/938 [00:29<00:00, 32.17it/s]\n",
      "V Loss:  4.9365, Avg Loss:  4.9461, Counter: 0, Best Loss:  4.9717: 100%|██████████| 28/28 [00:00<00:00, 75.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.4660, Avg Loss:  1.2137: 100%|██████████| 938/938 [00:29<00:00, 31.80it/s]\n",
      "V Loss:  4.9138, Avg Loss:  4.9715, Counter: 0, Best Loss:  4.9461: 100%|██████████| 28/28 [00:00<00:00, 75.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.8550, Avg Loss:  1.1707: 100%|██████████| 938/938 [00:29<00:00, 31.93it/s]\n",
      "V Loss:  5.2034, Avg Loss:  4.9926, Counter: 1, Best Loss:  4.9461: 100%|██████████| 28/28 [00:00<00:00, 75.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.1872, Avg Loss:  1.1324: 100%|██████████| 938/938 [00:29<00:00, 32.27it/s]\n",
      "V Loss:  4.8478, Avg Loss:  4.9966, Counter: 2, Best Loss:  4.9461: 100%|██████████| 28/28 [00:00<00:00, 76.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.8398, Avg Loss:  1.0971: 100%|██████████| 938/938 [00:30<00:00, 30.66it/s]\n",
      "V Loss:  5.0754, Avg Loss:  5.0414, Counter: 3, Best Loss:  4.9461: 100%|██████████| 28/28 [00:00<00:00, 75.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# model.fit(train_loader, dev_loader, EPOCHS, LEARNING_RATE, os.path.join(DIR, 'best_model.pth'))\n",
    "model.fit(train_loader, dev_loader, EPOCHS, LEARNING_RATE, OPTIMIZER, os.path.join(DIR, 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:  4.5516:   0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:  4.6065: 100%|██████████| 41/41 [00:00<00:00, 76.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:    0.07\n"
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "model.load_state_dict(torch.load(os.path.join(DIR, 'best_model.pth')))\n",
    "model.evaluate_metrics(test_loader, idx_to_words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep(config=None):\n",
    "    global LEARNING_RATE, DROPOUT, OPTIMIZER, NUM_LAYERS, NUM_HEADS\n",
    "    with wandb.init(config=config):\n",
    "        cfg = wandb.config\n",
    "        LEARNING_RATE = cfg['learning_rate']\n",
    "        DROPOUT = cfg['dropout']\n",
    "        OPTIMIZER = cfg['optimizer']\n",
    "        NUM_LAYERS = cfg['num_layers']\n",
    "        NUM_HEADS = cfg['num_heads']\n",
    "\n",
    "        train_dataset = TranslationDataset(train_en, train_fr, words_to_idx_en, words_to_idx_fr)\n",
    "        dev_dataset = TranslationDataset(dev_en, dev_fr, words_to_idx_en, words_to_idx_fr)\n",
    "        test_dataset = TranslationDataset(test_en, test_fr, words_to_idx_en, words_to_idx_fr)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model = Transformer(len(words_to_idx_en), len(words_to_idx_fr), EMBEDDING_DIM, NUM_HEADS, NUM_LAYERS, CONTEXT_SIZE, DROPOUT, filename=None).to(DEVICE)\n",
    "\n",
    "        print(summary(model, device=DEVICE))\n",
    "\n",
    "        model.fit(train_loader, dev_loader, EPOCHS, LEARNING_RATE, OPTIMIZER, os.path.join(DIR, 'best_model.pth'))\n",
    "\n",
    "        # load best model\n",
    "        model.load_state_dict(torch.load(os.path.join(DIR, 'best_model.pth')))\n",
    "\n",
    "        model.evaluate_metrics(test_loader, idx_to_words_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_id = wandb.sweep(cfg, project='Translation', entity='shu7bh')\n",
    "# wandb.agent(sweep_id, run_sweep, count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
